By Default Pod can commmunicate with each other even though they are on different nodes

# All You Need to Know About Certificates in Kubernetes
https://www.youtube.com/watch?v=gXz4cq3PKdg

# Kubernetes Components
https://kubernetes.io/docs/concepts/overview/components

# PKI certificates and requirements
https://kubernetes.io/docs/setup/best-practices/certificates

#Control ServiceAccount permissions using RBAC
# There are existing Namespaces ns1 and ns2 .

# Create ServiceAccount pipeline in both Namespaces.

# These SAs should be allowed to view almost everything in the whole cluster. You can use the default ClusterRole view for this.

# These SAs should be allowed to create and delete Deployments in their Namespace.

# Verify everything using kubectl auth can-i .


# RBAC Info

# Let's talk a little about RBAC resources:

# A ClusterRole|Role defines a set of permissions and where it is available, in the whole cluster or just a single Namespace.

# A ClusterRoleBinding|RoleBinding connects a set of permissions with an account and defines where it is applied, in the whole cluster or just a single Namespace.

# Because of this there are 4 different RBAC combinations and 3 valid ones:

# Role + RoleBinding (available in single Namespace, applied in single Namespace)
# ClusterRole + ClusterRoleBinding (available cluster-wide, applied cluster-wide)
# ClusterRole + RoleBinding (available cluster-wide, applied in single Namespace)
# Role + ClusterRoleBinding (NOT POSSIBLE: available in single Namespace, applied cluster-wide)

# Tip

# k get clusterrole view # there is default one

# k create clusterrole -h # examples

# k create rolebinding -h # examples

# k auth can-i delete deployments --as system:serviceaccount:ns1:pipeline -n ns1

# Solution

# # create Namespaces
# k -n ns1 create sa pipeline
# k -n ns2 create sa pipeline

# # use ClusterRole view
# k get clusterrole view # there is default one
# k create clusterrolebinding pipeline-view --clusterrole view --serviceaccount ns1:pipeline --serviceaccount ns2:pipeline

# # manage Deployments in both Namespaces
# k create clusterrole -h # examples
# k create clusterrole pipeline-deployment-manager --verb create,delete --resource deployments
# # instead of one ClusterRole we could also create the same Role in both Namespaces

# k -n ns1 create rolebinding pipeline-deployment-manager --clusterrole pipeline-deployment-manager --serviceaccount ns1:pipeline
# k -n ns2 create rolebinding pipeline-deployment-manager --clusterrole pipeline-deployment-manager --serviceaccount ns2:pipeline

# Verify

# # namespace ns1 deployment manager
# k auth can-i delete deployments --as system:serviceaccount:ns1:pipeline -n ns1 # YES
# k auth can-i create deployments --as system:serviceaccount:ns1:pipeline -n ns1 # YES
# k auth can-i update deployments --as system:serviceaccount:ns1:pipeline -n ns1 # NO
# k auth can-i update deployments --as system:serviceaccount:ns1:pipeline -n default # NO

# # namespace ns2 deployment manager
# k auth can-i delete deployments --as system:serviceaccount:ns2:pipeline -n ns2 # YES
# k auth can-i create deployments --as system:serviceaccount:ns2:pipeline -n ns2 # YES
# k auth can-i update deployments --as system:serviceaccount:ns2:pipeline -n ns2 # NO
# k auth can-i update deployments --as system:serviceaccount:ns2:pipeline -n default # NO

# # cluster wide view role
# k auth can-i list deployments --as system:serviceaccount:ns1:pipeline -n ns1 # YES
# k auth can-i list deployments --as system:serviceaccount:ns1:pipeline -A # YES
# k auth can-i list pods --as system:serviceaccount:ns1:pipeline -A # YES
# k auth can-i list pods --as system:serviceaccount:ns2:pipeline -A # YES
# k auth can-i list secrets --as system:serviceaccount:ns2:pipeline -A # NO (default view-role doesn't allow)

#Control User permissions using RBAC
# There is existing Namespace applications .

# User smoke should be allowed to create and delete Pods, Deployments and StatefulSets in Namespace applications.
# User smoke should have view permissions (like the permissions of the default ClusterRole named view ) in all Namespaces but not in kube-system .
# User smoke should be allowed to retrieve available Secret names in Namespace applications. Just the Secret names, no data.
# Verify everything using kubectl auth can-i .

# RBAC Info

# Let's talk a little about RBAC resources:

# A ClusterRole|Role defines a set of permissions and where it is available, in the whole cluster or just a single Namespace.

# A ClusterRoleBinding|RoleBinding connects a set of permissions with an account and defines where it is applied, in the whole cluster or just a single Namespace.

# Because of this there are 4 different RBAC combinations and 3 valid ones:

# Role + RoleBinding (available in single Namespace, applied in single Namespace)
# ClusterRole + ClusterRoleBinding (available cluster-wide, applied cluster-wide)
# ClusterRole + RoleBinding (available cluster-wide, applied in single Namespace)
# Role + ClusterRoleBinding (NOT POSSIBLE: available in single Namespace, applied cluster-wide)

# Tip


# # 1)
# k -n applications create role -h
# k -n applications create rolebinding -h

# # 2)
# # as of now it’s not possible to create deny-RBAC in K8s.
# # so we allow for all other namespaces

# # 3)
# # it's NOT POSSIBLE using plain K8s
# # see solution for more explanation

# # 4)
# k auth can-i -h
# k auth can-i create deployments --as smoke -n applications


# Solution

# ⁣1) RBAC for Namespace applications


# k -n applications create role smoke --verb create,delete --resource pods,deployments,sts
# k -n applications create rolebinding smoke --role smoke --user smoke

# ⁣2) view permission in all Namespaces but not kube-system


# As of now it’s not possible to create deny-RBAC in K8s

# So we allow for all other Namespaces


# k get ns # get all namespaces
# k -n applications create rolebinding smoke-view --clusterrole view --user smoke
# k -n default create rolebinding smoke-view --clusterrole view --user smoke
# k -n kube-node-lease create rolebinding smoke-view --clusterrole view --user smoke
# k -n kube-public create rolebinding smoke-view --clusterrole view --user smoke

# ⁣3) just list Secret names, no content


# This is NOT POSSIBLE using plain K8s RBAC. You might think of doing this:


# # NOT POSSIBLE: assigning "list" also allows user to read secret values
# k -n applications create role list-secrets --verb list --resource secrets

# k -n applications create rolebinding ...

# Having the list verb you can simply run kubectl get secrets -oyaml and see all content. Dangerous misconfiguration!


# Verify

# # applications
# k auth can-i create deployments --as smoke -n applications # YES
# k auth can-i delete deployments --as smoke -n applications # YES
# k auth can-i delete pods --as smoke -n applications # YES
# k auth can-i delete sts --as smoke -n applications # YES
# k auth can-i delete secrets --as smoke -n applications # NO
# k auth can-i list deployments --as smoke -n applications # YES
# k auth can-i list secrets --as smoke -n applications # NO
# k auth can-i get secrets --as smoke -n applications # NO

# # view in all namespaces but not kube-system
# k auth can-i list pods --as smoke -n default # YES
# k auth can-i list pods --as smoke -n applications # YES
# k auth can-i list pods --as smoke -n kube-public # YES
# k auth can-i list pods --as smoke -n kube-node-lease # YES
# k auth can-i list pods --as smoke -n kube-system # NO

#Create KEY and CSR

# The idea here is to create a new "user" that can communicate with K8s.

# For this now:

# Create a new KEY at /root/60099.key for user named 60099@internal.users
# Create a CSR at /root/60099.csr for the KEY

# Explanation

# Users in K8s are managed via CRTs and the CN/CommonName field in them. The cluster CA needs to sign these CRTs.

# This can be achieved with the following procedure:

# Create a KEY (Private Key) file
# Create a CSR (CertificateSigningRequest) file for that KEY
# Create a CRT (Certificate) by signing the CSR. Done using the CA (Certificate Authority) of the cluster

# Tip

# openssl genrsa -out XXX 2048

# openssl req -new -key XXX -out XXX

# Solution

# openssl genrsa -out 60099.key 2048

# openssl req -new -key 60099.key -out 60099.csr
# # set Common Name = 60099@internal.users

#for getting any unencrypted secret from etcd
# access secret int etcd
# cat /etc/kubernetes/manifests/kube-apiserver.yaml | grep etcd

# ETCDCTL_API=3 etcdctl --cert /etc/kubernetes/pki/apiserver-etcd-client.crt --key /etc/kubernetes/pki/apiserver-etcd-client.key --cacert /etc/kubernetes/pki/etcd/ca.crt endpoint health

# # --endpoints "https://127.0.0.1:2379" not necessary because we’re on same node

# ETCDCTL_API=3 etcdctl --cert /etc/kubernetes/pki/apiserver-etcd-client.crt --key /etc/kubernetes/pki/apiserver-etcd-client.key --cacert /etc/kubernetes/pki/etcd/ca.crt get /registry/secrets/default/secret1


# Enable ETCD Encryption
# Create an EncryptionConfiguration file at /etc/kubernetes/etcd/ec.yaml and make ETCD use it.
# One provider should be of type aesgcm with password this-is-very-sec . All new secrets should be encrypted using this one.
# One provider should be the identity one to still be able to read existing unencrypted secrets.

# Solution

# Generate EncryptionConfiguration:

# mkdir -p /etc/kubernetes/etcd
# echo -n this-is-very-sec | base64
# apiVersion: apiserver.config.k8s.io/v1
# kind: EncryptionConfiguration
# resources:
#   - resources:
#     - secrets
#     providers:
#     - aesgcm:
#         keys:
#         - name: key1
#           secret: dGhpcy1pcy12ZXJ5LXNlYw==
#     - identity: {}

# 1. Add a new volume and volumeMount in /etc/kubernetes/manifests/kube-apiserver.yaml, so that the container can access the file.

# 2. Pass the new file as argument: --encryption-provider-config=/etc/kubernetes/etcd/ec.yaml

# spec:
#   containers:
#   - command:
#     - kube-apiserver
# ...
#     - --encryption-provider-config=/etc/kubernetes/etcd/ec.yaml
# ...
#     volumeMounts:
#     - mountPath: /etc/kubernetes/etcd
#       name: etcd
#       readOnly: true
# ...
#   hostNetwork: true
#   priorityClassName: system-cluster-critical
#   volumes:
#   - hostPath:
#       path: /etc/kubernetes/etcd
#       type: DirectoryOrCreate
#     name: etcd
# ...

# Wait till apiserver was restarted:

# watch crictl ps



# Create a Pod named prime image nginx:alpine .

# The container should run as privileged .

# Install iptables (apk add iptables ) inside the Pod.

# Test the capabilities using iptables -L .


# Solution

# Generate Pod yaml


# k run prime --image=nginx:alpine -oyaml --dry-run=client --command -- sh -c 'sleep 1d' > pod.yaml

# Set the privileged :


# apiVersion: v1
# kind: Pod
# metadata:
#   labels:
#     run: prime
#   name: prime
# spec:
#   containers:
#   - command:
#     - sh
#     - -c
#     - sleep 1d
#     image: nginx:alpine
#     name: prime
#     securityContext:
#       privileged: true
#   dnsPolicy: ClusterFirst
#   restartPolicy: Always

# Now exec into the Pod and run apk add iptables .


# k exec prime -- apk add iptables

# You'll see that iptables -L needs capabilities to run which it here gets through privileged.


# k exec prime -- iptables -L

####ImagePolicyWebhook Setup###
# An ImagePolicyWebhook setup has been half finished, complete it:

# Make sure admission_config.json points to correct kubeconfig
# Set the allowTTL to 100
# All Pod creation should be prevented if the external service is not reachable
# The external service will be reachable under https://localhost:1234 in the future. It doesn't exist yet so it shouldn't be able to create any Pods till then
# Register the correct admission plugin in the apiserver

# Solution

# The /etc/kubernetes/policywebhook/admission_config.json should look like this:

# {
#    "apiVersion": "apiserver.config.k8s.io/v1",
#    "kind": "AdmissionConfiguration",
#    "plugins": [
#       {
#          "name": "ImagePolicyWebhook",
#          "configuration": {
#             "imagePolicy": {
#                "kubeConfigFile": "/etc/kubernetes/policywebhook/kubeconf",
#                "allowTTL": 100,
#                "denyTTL": 50,
#                "retryBackoff": 500,
#                "defaultAllow": false
#             }
#          }
#       }
#    ]
# }

# The /etc/kubernetes/policywebhook/kubeconf should contain the correct server: 

# apiVersion: v1
# kind: Config
# clusters:
# - cluster:
#     certificate-authority: /etc/kubernetes/policywebhook/external-cert.pem
#     server: https://localhost:1234
#   name: image-checker
# ...

# The apiserver needs to be configured with the ImagePolicyWebhook admission plugin:


# spec:
#   containers:
#   - command:
#     - kube-apiserver
#     - --enable-admission-plugins=NodeRestriction,ImagePolicyWebhook
#     - --admission-control-config-file=/etc/kubernetes/policywebhook/admission_config.json

# Luckily the --admission-control-config-file argument seems already to be configured.



# Test your Solution

# Wait till apiserver container restarted:

# watch crictl ps

# To test your solution you can simply try to create a Pod:

# k run pod --image=nginx

# It should throw you an error like:


# Error from server (Forbidden): pods "pod" is forbidden: Post "https://localhost:1234/?timeout=30s": dial tcp 127.0.0.1:1234: connect: connection refused

# Once that service would be implemented and if it would allow the Pod, the Pod could be created.

